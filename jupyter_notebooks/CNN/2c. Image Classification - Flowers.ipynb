{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x182yUaaGOIx"
   },
   "source": [
    "### Download Flowers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "USgEKN9GGf60"
   },
   "outputs": [],
   "source": [
    "# #You can download the data manually as well instead of using 'wget'\n",
    "# !wget http://download.tensorflow.org/example_images/flower_photos.tgz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "3cCPVwdxpDvR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 643176\n",
      "-rw-rw-r--@ 1 vidyadharbendre  staff      32125 Mar 22 20:05 1a. Classification_MNIST_CNN_Keras.ipynb\n",
      "-rw-r--r--@ 1 vidyadharbendre  staff      80277 Mar 22 22:09 1b. Classification_MNIST_CNN_Keras_Functional.ipynb\n",
      "-rw-rw-r--@ 1 vidyadharbendre  staff    1269188 Mar 22 21:07 2a. Visualize an Image.ipynb\n",
      "-rw-------@ 1 vidyadharbendre  staff      49116 Mar 23 07:54 2b. Image_Classification_CNN.ipynb\n",
      "-rw-rw-r--@ 1 vidyadharbendre  staff      41563 Mar 23 10:13 2c. Image Classification - Flowers.ipynb\n",
      "drwxr-xr-x@ 5 vidyadharbendre  staff        160 Mar 22 20:54 \u001b[34mcheck_point_folder\u001b[m\u001b[m\n",
      "drwxr-x---@ 8 vidyadharbendre  staff        256 Feb 11  2016 \u001b[34mflower_photos\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 vidyadharbendre  staff  228813984 Feb 11  2016 flower_photos.tgz\n",
      "-rw-r--r--@ 1 vidyadharbendre  staff   77390704 Mar 23 10:13 flowers.h5\n",
      "-rw-r--r--@ 1 vidyadharbendre  staff    4842280 Mar 22 22:06 mnist_functional_template_v3.h5\n",
      "drwxr-xr-x@ 6 vidyadharbendre  staff        192 Mar 22 20:54 \u001b[34msave_mode_folder\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "#Check if file is downloaded\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "r9NeLYrOpIc0"
   },
   "outputs": [],
   "source": [
    "# #Unzip the data\n",
    "# !tar -xf flower_photos.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "fd2Zz_mYrU7N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 824\n",
      "-rw-r-----@   1 vidyadharbendre  staff  418049 Feb  9  2016 LICENSE.txt\n",
      "drwx------@ 635 vidyadharbendre  staff   20320 Feb 11  2016 \u001b[34mdaisy\u001b[m\u001b[m\n",
      "drwx------@ 900 vidyadharbendre  staff   28800 Feb 11  2016 \u001b[34mdandelion\u001b[m\u001b[m\n",
      "drwx------@ 643 vidyadharbendre  staff   20576 Feb 11  2016 \u001b[34mroses\u001b[m\u001b[m\n",
      "drwx------@ 701 vidyadharbendre  staff   22432 Feb 11  2016 \u001b[34msunflowers\u001b[m\u001b[m\n",
      "drwx------@ 801 vidyadharbendre  staff   25632 Feb 11  2016 \u001b[34mtulips\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "#Check how data is organized\n",
    "!ls -l flower_photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz0v83zDhs2y"
   },
   "source": [
    "### Build batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NumPy library, commonly aliased as 'np', for numerical computations and array operations.\n",
    "import numpy as np\n",
    "\n",
    "# Import the Pandas library, commonly aliased as 'pd', for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the Matplotlib library, specifically the pyplot module, for data visualization.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-da8Sz_BpnpI"
   },
   "outputs": [],
   "source": [
    "# Import the TensorFlow library, commonly aliased as 'tf', for building and training neural networks.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import the MNIST dataset from the TensorFlow Keras datasets module.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Import the to_categorical function from the TensorFlow Keras utils module.\n",
    "# This function is used for one-hot encoding of categorical labels.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Import the clear_session function from the TensorFlow Keras backend module.\n",
    "# This function is used to clear the Keras session and reset the TensorFlow graph.\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# Import necessary libraries and functions from TensorFlow.\n",
    "\n",
    "# Import the Sequential and Model classes from the TensorFlow Keras models module.\n",
    "# These classes are used to create sequential and functional API models, respectively.\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "\n",
    "# Import various layer classes (Reshape, Dense, BatchNormalization, LeakyReLU, Input)\n",
    "# from the TensorFlow Keras layers module.\n",
    "# These classes are used to define the architecture of neural network models.\n",
    "from tensorflow.keras.layers import Input, Reshape, Flatten, Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, LeakyReLU\n",
    "\n",
    "# Import callback classes (ModelCheckpoint, EarlyStopping) from the TensorFlow Keras callbacks module.\n",
    "# These classes are used to define callbacks for monitoring and controlling the training process.\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "dQQ7Oksaq7tI"
   },
   "outputs": [],
   "source": [
    "#Define some parameters\n",
    "img_size = 60\n",
    "img_depth = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBkyCTo1qWMy"
   },
   "source": [
    "Create an ImageDataGenerator object, it can also split data between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "bkUsCc6zp7Kp"
   },
   "outputs": [],
   "source": [
    "#ImageDataGenerator declaration with 20% data as test (80% for training)\n",
    "img_generator= tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3BVmtXdrHyh"
   },
   "source": [
    "ImageDataGenerator can read images directory and also resize them if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "IkCVDrPOqDjE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2939 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Build training generator. \n",
    "train_generator = img_generator.flow_from_directory('flower_photos',\n",
    "                                                    batch_size=64,\n",
    "                                                    target_size=(img_size, img_size),\n",
    "                                                    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "8Z7_0KoJGRDM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 731 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#Build test generator\n",
    "test_generator = img_generator.flow_from_directory('flower_photos',\n",
    "                                                   target_size=(img_size, img_size),                                                   \n",
    "                                                   subset='validation',\n",
    "                                                   batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ZnOY195Pt7mn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.preprocessing.image.DirectoryIterator"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCpXm9s4rjM1"
   },
   "source": [
    "ImageDataGenerator returns 64 images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "aUhNi9Krrpq7"
   },
   "outputs": [],
   "source": [
    "#Lets check the features (images) and Labels (flower class) returned by ImageDataGenerator\n",
    "X, y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "g_QIPMTvsZZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape (64, 60, 60, 3)\n",
      "Actual labels shape (64, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Input features shape', X.shape)\n",
    "print('Actual labels shape', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "1NUb1CcQx_Zo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "_iJgvxvh32gt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
       "       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,\n",
       "       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,\n",
       "       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,\n",
       "       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n",
       "       176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186.,\n",
       "       187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197.,\n",
       "       198., 199., 200., 201., 202., 203., 204., 205., 206., 207., 208.,\n",
       "       209., 210., 211., 212., 213., 214., 215., 216., 217., 218., 219.,\n",
       "       220., 221., 222., 223., 224., 225., 226., 227., 228., 229., 230.,\n",
       "       231., 232., 233., 234., 235., 236., 237., 238., 239., 240., 241.,\n",
       "       242., 243., 244., 245., 246., 247., 248., 249., 250., 251., 252.,\n",
       "       253., 254., 255.], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "es0UPokXxIKM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "xDV8kVGpOi_w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[111., 141., 167.],\n",
       "        [116., 137., 166.],\n",
       "        [116., 137., 166.],\n",
       "        ...,\n",
       "        [113., 128., 157.],\n",
       "        [113., 128., 157.],\n",
       "        [114., 129., 158.]],\n",
       "\n",
       "       [[117., 136., 166.],\n",
       "        [118., 139., 168.],\n",
       "        [117., 141., 169.],\n",
       "        ...,\n",
       "        [116., 131., 160.],\n",
       "        [118., 132., 161.],\n",
       "        [116., 130., 159.]],\n",
       "\n",
       "       [[118., 139., 168.],\n",
       "        [120., 141., 170.],\n",
       "        [120., 141., 170.],\n",
       "        ...,\n",
       "        [116., 131., 160.],\n",
       "        [117., 133., 159.],\n",
       "        [114., 131., 157.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 20.,  30.,   5.],\n",
       "        [ 33.,  36.,  15.],\n",
       "        [ 68.,  56.,   4.],\n",
       "        ...,\n",
       "        [ 36.,  33.,  16.],\n",
       "        [ 35.,  36.,  18.],\n",
       "        [ 31.,  34.,  13.]],\n",
       "\n",
       "       [[ 65.,  42.,   0.],\n",
       "        [ 74.,  59.,   4.],\n",
       "        [ 58.,  45.,  10.],\n",
       "        ...,\n",
       "        [ 43.,  45.,  24.],\n",
       "        [ 80.,  87.,  45.],\n",
       "        [ 20.,  22.,   9.]],\n",
       "\n",
       "       [[ 68.,  45.,  11.],\n",
       "        [ 66.,  56.,  21.],\n",
       "        [ 29.,  33.,  10.],\n",
       "        ...,\n",
       "        [ 22.,  26.,  12.],\n",
       "        [113., 111.,  62.],\n",
       "        [ 58.,  69.,  29.]]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1DOwu8Bhs29"
   },
   "source": [
    "### Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "tWQJ4SzZhs2-"
   },
   "outputs": [],
   "source": [
    "# #Clear any previous model from memory\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# #Initialize model\n",
    "# model = tf.keras.models.Sequential()\n",
    "\n",
    "# #normalize data\n",
    "# model.add(tf.keras.layers.BatchNormalization(input_shape=(img_size,img_size,3,)))\n",
    "\n",
    "# #Add Conv Layer\n",
    "# model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "# #normalize data\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# #Add Conv Layer\n",
    "# model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "\n",
    "# #normalize data\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# #Add Max Pool layer\n",
    "# model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# #Add Dense Layers after flattening the data\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "# #Add Dropout\n",
    "# model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "# #Add Output Layer\n",
    "# model.add(tf.keras.layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### version -3 ###\n",
    "# Clear any existing TensorFlow graph/session.\n",
    "clear_session()\n",
    "\n",
    "# Define the input layer with a shape of (28, 28, 1), representing 28x28 images with 1 channel.\n",
    "inputs = Input(shape=(img_size, img_size, 3))\n",
    "input_batch_norm_1 = BatchNormalization()(inputs)\n",
    "# Add a Conv2D layer with 32 filters, a kernel size of 3x3, ReLU activation function, and batch normalization.\n",
    "conv_layer_1 = Conv2D(32, kernel_size=(3, 3), activation=\"relu\", strides=(1,1), kernel_initializer='he_normal', name=\"conv_layer_1\")(input_batch_norm_1)\n",
    "conv_batch_norm_2 = BatchNormalization()(conv_layer_1)\n",
    "\n",
    "# Add a Conv2D layer with 64 filters, a kernel size of 3x3, ReLU activation function, and batch normalization.\n",
    "conv_layer_2 = Conv2D(64, kernel_size=(3, 3), activation=\"relu\", name=\"conv_layer_2\")(conv_batch_norm_2)\n",
    "conv_batch_norm_2 = BatchNormalization()(conv_layer_2)\n",
    "\n",
    "# Add MaxPooling layer\n",
    "max_pooling_1 = MaxPool2D(pool_size=(2,2), strides=2)(conv_batch_norm_2)\n",
    "\n",
    "# Flatten the output of the convolutional layer.\n",
    "flattened = Flatten()(max_pooling_1)\n",
    "\n",
    "# Define the architecture of the neural network using fully connected (Dense) layers.\n",
    "# Reshape data from 2D to 1D -> 28x28 to 784\n",
    "\n",
    "# Add the first hidden layer with 128 neurons, ReLU activation function, and batch normalization.\n",
    "first_hidden_layer = Dense(128, activation=\"relu\", name=\"first_hidden_layer\")(flattened)\n",
    "#first_batch_norm = BatchNormalization()(first_hidden_layer)\n",
    "first_drop = Dropout(0.25)(first_hidden_layer)\n",
    "\n",
    "\n",
    "# Add the output layer with 10 neurons (for 10 classes) and softmax activation function.\n",
    "outputs = Dense(5, activation='softmax')(first_drop)\n",
    "\n",
    "# Define the model with input and output layers.\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "GsuEXbofhs3D"
   },
   "outputs": [],
   "source": [
    "# Compile the model with stochastic gradient descent (SGD) optimizer, categorical crossentropy loss,\n",
    "# and accuracy metric for evaluation during training.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "KCB11Qm44ynY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60, 60, 3)]       0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 60, 60, 3)        12        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv_layer_1 (Conv2D)       (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 58, 58, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_2 (Conv2D)       (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 56, 56, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " first_hidden_layer (Dense)  (None, 128)               6422656   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,443,089\n",
      "Trainable params: 6,442,891\n",
      "Non-trainable params: 198\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "0V1E9Ua1tUPP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60, 60, 3)]       0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 60, 60, 3)        12        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv_layer_1 (Conv2D)       (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 58, 58, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_layer_2 (Conv2D)       (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 56, 56, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " first_hidden_layer (Dense)  (None, 128)               6422656   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,443,089\n",
      "Trainable params: 6,442,891\n",
      "Non-trainable params: 198\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v7UlgC5hs3g"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cklB73X8yCvt"
   },
   "source": [
    "In the context of machine learning, both model checkpoints and callbacks are tools used to improve the training process of neural networks, particularly in deep learning frameworks like TensorFlow and PyTorch. However, they serve different purposes:\n",
    "\n",
    "Model Checkpoint:\n",
    "\n",
    "A model checkpoint is a feature used to save the model's current weights or entire state at certain intervals during training.\n",
    "The primary purpose of a model checkpoint is to save the model's progress periodically so that it can be restored or used later for inference or further training.\n",
    "Model checkpoints are typically used to save the best-performing model based on a specified metric, such as validation accuracy or loss. This ensures that you retain the best version of the model throughout the training process.\n",
    "Checkpoints are especially useful for long training processes where interruptions or failures may occur, allowing you to resume training from the last saved checkpoint rather than starting from scratch.\n",
    "Callback:\n",
    "\n",
    "A callback is a function or set of functions that are executed at specific points during the training process, such as at the beginning or end of an epoch or after a batch of data is processed.\n",
    "Callbacks are more versatile and can perform various tasks during training, such as logging training metrics, adjusting learning rates dynamically, implementing early stopping, or saving model checkpoints.\n",
    "While model checkpoints are a specific type of callback used for saving model weights, callbacks can perform a wide range of additional functions to customize the training process according to your specific needs.\n",
    "Callbacks can be defined by the user or provided by the deep learning framework as pre-defined functions. Users can also create custom callbacks to implement specific behaviors not covered by built-in callbacks.\n",
    "In summary, a model checkpoint is a type of callback used specifically for saving model weights or state during training, while callbacks encompass a broader range of functions that can be executed at various stages of the training process to customize and enhance the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'flowers.h5',  # File path to save the model\n",
    "    save_best_only=True,       # Save only the best model based on monitored metric\n",
    "    monitor='val_accuracy',    # Metric to monitor for determining the best model\n",
    "    mode='max',                # Mode for determining the best model ('max' or 'min')\n",
    "    verbose=1                  # Verbosity mode (0 or 1)\n",
    ")\n",
    "\n",
    "# EarlyStopping callback to stop training when the monitored metric stops improving\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Metric to monitor for early stopping\n",
    "    patience=5,                # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,                 # Verbosity mode (0 or 1)\n",
    "    restore_best_weights=True  # Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "ldJt7GHB5dtQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2939//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "7YGqTxKQhs3l"
   },
   "outputs": [],
   "source": [
    "# model.fit(train_generator,\n",
    "#           epochs=200,\n",
    "#           steps_per_epoch= 2939//64,  #Number of batches per epoch\n",
    "#           validation_data=test_generator,\n",
    "#           validation_steps = 731//64, \n",
    "#           callbacks=[model_checkpoint]) #Number of test images//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 10:13:59.731571: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - ETA: 0s - loss: 4.9682 - accuracy: 0.3558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 10:14:02.454355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.28267, saving model to flowers.h5\n",
      "45/45 [==============================] - 4s 72ms/step - loss: 4.9682 - accuracy: 0.3558 - val_loss: 1.5671 - val_accuracy: 0.2827\n",
      "Epoch 2/2\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.4507 - accuracy: 0.4240\n",
      "Epoch 2: val_accuracy improved from 0.28267 to 0.33523, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 68ms/step - loss: 1.4507 - accuracy: 0.4240 - val_loss: 1.4678 - val_accuracy: 0.3352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bd651520>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the training data (X_train and y_train) with the following configurations:\n",
    "# - Validation data: (X_test, y_test), used to evaluate the model after each epoch.\n",
    "# - Number of epochs: 2, specifying how many times the entire training dataset is passed through the model.\n",
    "# - Batch size: 32, indicating the number of samples per gradient update during training.\n",
    "# - Callbacks: ModelCheckpoint and EarlyStopping, used for saving the best model and stopping early to prevent overfitting.\n",
    "model.fit(train_generator,\n",
    "          epochs=2,\n",
    "          steps_per_epoch= 2939//64,  #Number of batches per epoch          \n",
    "          validation_data=test_generator,\n",
    "          validation_steps = 731//64,\n",
    "          callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_epoch=5,  # Start training from the 3th epoch (indexing starts from 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.2553 - accuracy: 0.4515\n",
      "Epoch 3: val_accuracy improved from 0.33523 to 0.43324, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 70ms/step - loss: 1.2553 - accuracy: 0.4515 - val_loss: 1.3864 - val_accuracy: 0.4332\n",
      "Epoch 4/10\n",
      "44/45 [============================>.] - ETA: 0s - loss: 1.1341 - accuracy: 0.5233\n",
      "Epoch 4: val_accuracy improved from 0.43324 to 0.46449, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 68ms/step - loss: 1.1321 - accuracy: 0.5249 - val_loss: 1.3564 - val_accuracy: 0.4645\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 1.0343 - accuracy: 0.5704\n",
      "Epoch 5: val_accuracy improved from 0.46449 to 0.49006, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 68ms/step - loss: 1.0343 - accuracy: 0.5704 - val_loss: 1.3861 - val_accuracy: 0.4901\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.9256 - accuracy: 0.6160\n",
      "Epoch 6: val_accuracy improved from 0.49006 to 0.55540, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 68ms/step - loss: 0.9256 - accuracy: 0.6160 - val_loss: 1.3436 - val_accuracy: 0.5554\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.8333 - accuracy: 0.6525\n",
      "Epoch 7: val_accuracy improved from 0.55540 to 0.55824, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 67ms/step - loss: 0.8333 - accuracy: 0.6525 - val_loss: 1.4331 - val_accuracy: 0.5582\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7496 - accuracy: 0.6786\n",
      "Epoch 8: val_accuracy did not improve from 0.55824\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.7496 - accuracy: 0.6786 - val_loss: 1.4858 - val_accuracy: 0.5469\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.7380 - accuracy: 0.6870\n",
      "Epoch 9: val_accuracy did not improve from 0.55824\n",
      "45/45 [==============================] - 3s 66ms/step - loss: 0.7380 - accuracy: 0.6870 - val_loss: 1.5329 - val_accuracy: 0.5398\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6261 - accuracy: 0.7475\n",
      "Epoch 10: val_accuracy did not improve from 0.55824\n",
      "45/45 [==============================] - 3s 65ms/step - loss: 0.6261 - accuracy: 0.7475 - val_loss: 1.3884 - val_accuracy: 0.5355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2be323220>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          steps_per_epoch= 2939//64,  #Number of batches per epoch          \n",
    "          validation_data=test_generator,\n",
    "          validation_steps = 731//64,\n",
    "          initial_epoch=2,\n",
    "          callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6152 - accuracy: 0.7464\n",
      "Epoch 11: val_accuracy did not improve from 0.55824\n",
      "45/45 [==============================] - 3s 69ms/step - loss: 0.6152 - accuracy: 0.7464 - val_loss: 1.5430 - val_accuracy: 0.5582\n",
      "Epoch 12/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.7652\n",
      "Epoch 12: val_accuracy improved from 0.55824 to 0.56534, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 71ms/step - loss: 0.5859 - accuracy: 0.7652 - val_loss: 1.5193 - val_accuracy: 0.5653\n",
      "Epoch 13/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.7868\n",
      "Epoch 13: val_accuracy improved from 0.56534 to 0.57102, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 70ms/step - loss: 0.5154 - accuracy: 0.7868 - val_loss: 1.4632 - val_accuracy: 0.5710\n",
      "Epoch 14/25\n",
      "44/45 [============================>.] - ETA: 0s - loss: 0.5292 - accuracy: 0.8008\n",
      "Epoch 14: val_accuracy did not improve from 0.57102\n",
      "45/45 [==============================] - 3s 66ms/step - loss: 0.5294 - accuracy: 0.8014 - val_loss: 1.6115 - val_accuracy: 0.5611\n",
      "Epoch 15/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4457 - accuracy: 0.8191\n",
      "Epoch 15: val_accuracy did not improve from 0.57102\n",
      "45/45 [==============================] - 3s 68ms/step - loss: 0.4457 - accuracy: 0.8191 - val_loss: 1.6932 - val_accuracy: 0.5625\n",
      "Epoch 16/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.4363 - accuracy: 0.8254\n",
      "Epoch 16: val_accuracy improved from 0.57102 to 0.60369, saving model to flowers.h5\n",
      "45/45 [==============================] - 3s 68ms/step - loss: 0.4363 - accuracy: 0.8254 - val_loss: 1.4652 - val_accuracy: 0.6037\n",
      "Epoch 17/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8473\n",
      "Epoch 17: val_accuracy did not improve from 0.60369\n",
      "45/45 [==============================] - 3s 67ms/step - loss: 0.3807 - accuracy: 0.8473 - val_loss: 1.6562 - val_accuracy: 0.5852\n",
      "Epoch 18/25\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8497\n",
      "Epoch 18: val_accuracy did not improve from 0.60369\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "45/45 [==============================] - 3s 67ms/step - loss: 0.3612 - accuracy: 0.8497 - val_loss: 1.7961 - val_accuracy: 0.5653\n",
      "Epoch 18: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2be98d5e0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          epochs=25,\n",
    "          steps_per_epoch= 2939//64,  #Number of batches per epoch          \n",
    "          validation_data=test_generator,\n",
    "          validation_steps = 731//64,\n",
    "          initial_epoch=10,\n",
    "          callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2b. Image Classification - Flowers.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
