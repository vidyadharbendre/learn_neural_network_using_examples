{"cells":[{"cell_type":"markdown","metadata":{"id":"hKxFOKUUo_0e"},"source":["#### Downlad the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3sRwJ_mo49J"},"outputs":[],"source":["#Download both images and annotations\n","!wget -q http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n","!wget -q http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aikHskRpWE8"},"outputs":[],"source":["#Check current directory to make sure data is downloaded\n","!ls -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXSZi1law-BX"},"outputs":[],"source":["#unzip the tar files downloaded abve\n","!tar xf images.tar.gz\n","!tar xf annotations.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvrVCc0bxSvi"},"outputs":[],"source":["#Explore directories\n","!ls -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVRcONh64ESo"},"outputs":[],"source":["!ls -l images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myD1MThUbylY"},"outputs":[],"source":["!ls -l images | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBuZuw2WaQVx"},"outputs":[],"source":["!ls -l annotations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLXGogOMb4cb"},"outputs":[],"source":["!ls -l annotations/xmls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3pRBcHsRBOMq"},"outputs":[],"source":["#Check the xml annotations\n","!ls -l annotations/xmls | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yj7Qh4NGgET"},"outputs":[],"source":["!cat annotations/xmls/yorkshire_terrier_18.xml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUfVehWP0WLj"},"outputs":[],"source":["#Install tidy to review xml files\n","!sudo apt-get install tidy --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXWETTacxV4h"},"outputs":[],"source":["#Check one of the xml file to understand annotations\n","!tidy -xml -i annotations/xmls/wheaten_terrier_170.xml"]},{"cell_type":"markdown","metadata":{"id":"rDL5UxUs0nUu"},"source":["#### Convert XML to CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5b5pJF20r3k"},"outputs":[],"source":["#Mount Google drive (change code for local machine). We need to copy generate_dataset.py script to current directory\n","from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wh8IQWfZ-cnJ"},"outputs":[],"source":["#Copy generate_dataset.py file to current directory\n","!cp \"/gdrive/My Drive/ACV/Localization/generate_dataset.py\" ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8pqOIYVE8ty"},"outputs":[],"source":["!ls -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZuckkwA1gHb"},"outputs":[],"source":["#Move all xml files to images folder, this is needed for python script used next\n","!mv annotations/xmls/* images/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZqrnN7t1nCv"},"outputs":[],"source":["#Build csv file for both training and test dataset\n","!python generate_dataset.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZirB08cx2SBr"},"outputs":[],"source":["!ls -l"]},{"cell_type":"markdown","metadata":{"id":"KIN4CgX185lH"},"source":["#### Visualize Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JvG_JZf88KP"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import cv2\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98RE4JSU2bdF"},"outputs":[],"source":["#Read csv file as pandas dataframe, csv file has no header\n","train_df = pd.read_csv('train.csv', header=None, \n","                       names=['File', 'Height','Width','xmin',\n","                              'ymin','xmax', 'ymax','Class','Label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y-U19sPMCQeV"},"outputs":[],"source":["print(train_df.shape)\n","train_df.sample(n=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBQRF_RC4Pzc"},"outputs":[],"source":["#Create a dictionary to hold label and corresponding class name\n","num_classes = train_df['Label'].unique()\n","label_class_dict = dict(zip(train_df['Label'], train_df['Class']))\n","#label_class_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdex_ymSFhc4"},"outputs":[],"source":["num_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2A6QVyV_T1yp"},"outputs":[],"source":["len(num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-56OxVKeGbzo"},"outputs":[],"source":["label_class_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iemaa5bXWzg"},"outputs":[],"source":["train_df.shape"]},{"cell_type":"markdown","metadata":{"id":"wTi8vgryCqh4"},"source":["Show images with bounding box"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J-p3ne1E95Ru"},"outputs":[],"source":["#Pickup a random image number\n","img_num = np.random.randint(0, train_df.shape[0])\n","img_num = 2848\n","#Read the image and draw a rectangle as per bounding box information\n","img = cv2.imread(train_df.loc[img_num,'File'])\n","img = cv2.resize(img,(224, 224))\n","w = train_df.loc[img_num, 'Width']\n","h = train_df.loc[img_num, 'Height']\n","x_ratio = 224/w\n","y_ratio = 224/h\n","\n","cv2.rectangle(img, \n","             (int(train_df.loc[img_num, 'xmin']*x_ratio),int(train_df.loc[img_num, 'ymin']*y_ratio)),\n","             (int(train_df.loc[img_num, 'xmax']*x_ratio),int(train_df.loc[img_num, 'ymax']*y_ratio)), \n","             (0,255,0),\n","             2)\n","\n","#Convert BGR format (used by opencv to RGB format used by matplotlib)\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","#Draw image using matplotlib\n","plt.figure(figsize=(10,10))\n","plt.suptitle(train_df.loc[img_num, 'Class'])\n","plt.imshow(img)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zZ1iGShfpWb"},"outputs":[],"source":["img_num"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fr2d9x6o8ujS"},"outputs":[],"source":["train_df.loc[img_num, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NOnn8CixMEcw"},"outputs":[],"source":["#Read the validation csv file\n","test_df = pd.read_csv('validation.csv', header=None, \n","                       names=['File', 'Height','Width','xmin',\n","                              'ymin','xmax', 'ymax','Class','Label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yON7EYPHTa7w"},"outputs":[],"source":["train_df.shape, test_df.shape"]},{"cell_type":"markdown","metadata":{"id":"ONbePE8pDUap"},"source":["#### Build a Batch Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJHj-NNvEhpa"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8RGtJJKEmPK"},"outputs":[],"source":["img_size = 224"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG9qxvqfDXgJ"},"outputs":[],"source":["def batch_generator(df, batch_size=32):\n","\n","    while True:\n","\n","        #Create indexes\n","        image_nums = np.random.randint(0,df.shape[0], size=batch_size)\n","\n","        #Create empty arrays\n","        #1. To hold image input\n","        batch_images = np.zeros(shape=(batch_size, img_size, img_size, 3))\n","\n","        #Classification Labels \n","        batch_labels = np.zeros(shape=(batch_size, len(num_classes)))\n","        \n","        #Regression labels - 4 numbers per example image\n","        batch_bboxes = np.zeros(shape=(batch_size, 4))\n","        \n","\n","        for i in range(batch_size):\n","\n","            #Read image and resize\n","            img = tf.keras.preprocessing.image.load_img(df.loc[image_nums[i], 'File'], \n","                                                        target_size=(img_size, img_size))\n","            \n","            #Convert to numpy array\n","            img_array = tf.keras.preprocessing.image.img_to_array(img)\n","\n","            #Update batch\n","            batch_images[i] = img_array\n","\n","            #Read image classification label & convert to one hot vector\n","            cl_label = df.loc[image_nums[i], 'Label']\n","            cl_label = tf.keras.utils.to_categorical(cl_label, num_classes=len(num_classes))\n","            batch_labels[i] = cl_label\n","\n","            #Read and resize bounding box co-ordinates\n","            img_width = df.loc[image_nums[i], 'Width']\n","            img_height = df.loc[image_nums[i], 'Height']\n","            \n","            xmin = df.loc[image_nums[i], 'xmin'] * img_size/img_width\n","            xmax = df.loc[image_nums[i], 'xmax'] * img_size/img_width\n","\n","            ymin = df.loc[image_nums[i], 'ymin'] * img_size/img_height\n","            ymax = df.loc[image_nums[i], 'ymax'] * img_size/img_height\n","\n","            #We will ask model to predict xmin, ymin, width and height of bounding box\n","            batch_bboxes[i] = [xmin, ymin, xmax-xmin, ymax-ymin]\n","\n","        #Normalize batch images as per Pre-trained model to be used\n","        for i in range(batch_size):\n","            batch_images[i] = tf.keras.applications.resnet50.preprocess_input(batch_images[i])\n","        \n","        #Make bounding boxes (x, y, w, h) as numbers between 0 and 1 - this seems to work better\n","        batch_bboxes = batch_bboxes /img_size\n","\n","        #Return batch - use yield function to make it a python generator\n","        yield batch_images, [batch_labels, batch_bboxes]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2Fw6l71Sig-"},"outputs":[],"source":["gen = batch_generator(train_df, batch_size=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9Hxm4_c075t"},"outputs":[],"source":["gen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cylN-wK8SqEB"},"outputs":[],"source":["X, y = next(gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtOyEuKWvqKE"},"outputs":[],"source":["print(X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNIBLPKI1KUJ"},"outputs":[],"source":["X[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9LebH2slKf3"},"outputs":[],"source":["y[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1sE1AfSq_yK"},"outputs":[],"source":["#Classification\n","y[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nF5WvnOy_aTX"},"outputs":[],"source":["y[1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5a2l4Gv8lTXb"},"outputs":[],"source":["#Regression\n","y[1][0]"]},{"cell_type":"markdown","metadata":{"id":"sFGPcGaMJt9-"},"source":["#### Build the Model"]},{"cell_type":"markdown","metadata":{"id":"MVXicG2xKBoq"},"source":["Load Pre-Trained Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBPc2TXuJn5m"},"outputs":[],"source":["tf.keras.backend.clear_session()\n","model = tf.keras.applications.ResNet50(include_top=False, #Do not include FC layer at the end\n","                                       input_shape=(img_size, img_size, 3),\n","                                       weights='imagenet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8b6OeTiEEOa"},"outputs":[],"source":["model.output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvPcn5zmv0H5"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Z30I35deKR2_"},"source":["Freeze all layers of Pre-trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_G00NdDiMLkx"},"outputs":[],"source":["len(model.layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nsafx9zlKEzA"},"outputs":[],"source":["for layer in model.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itYAVviBgJW4"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s77uFZ5gh-25"},"outputs":[],"source":["model.output"]},{"cell_type":"markdown","metadata":{"id":"ochVcQl_Knjg"},"source":["Add layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrdSJfSFKXIH"},"outputs":[],"source":["#get Output layer of Pre-trained model\n","x1 = model.output\n","\n","#Flatten the output to feed to Dense layer\n","#x2 = tf.keras.layers.GlobalAveragePooling2D()(x1)\n","x2 = tf.keras.layers.Conv2D(32, (1,1), activation='relu')(x1)\n","\n","#Add Dropout\n","x3 = tf.keras.layers.Dropout(0.5)(x2)\n","\n","#Add one Dense layer\n","x4 = tf.keras.layers.Flatten()(x3)\n","#x5 = tf.keras.layers.Dense(100, activation='relu')(x4)\n","\n","#Batch Norm\n","x6 = tf.keras.layers.BatchNormalization()(x4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Utn0mVPeTHZA"},"outputs":[],"source":["x2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwRuCC7gTKTv"},"outputs":[],"source":["x3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgQBof80TZZ0"},"outputs":[],"source":["x4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rpK9VN1TfTl"},"outputs":[],"source":["x5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mm-a5uXDkuwR"},"outputs":[],"source":["x6"]},{"cell_type":"markdown","metadata":{"id":"dSsesgHCKuq0"},"source":["Build layer for Classification Label output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUyBwuxlKt67"},"outputs":[],"source":["#Classification\n","label_output = tf.keras.layers.Dense(len(num_classes), \n","                                     activation='softmax', \n","                                     name='class_op')(x6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bksL8On7tZ5f"},"outputs":[],"source":["label_output"]},{"cell_type":"markdown","metadata":{"id":"smOcT5VLK6BA"},"source":["Build layer for bounding box output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ujk5IXzIK9O1"},"outputs":[],"source":["#Regression\n","bbox_output = tf.keras.layers.Dense(4, \n","                                    activation='sigmoid', \n","                                    name='reg_op')(x6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9roF0YmBuQ4a"},"outputs":[],"source":["bbox_output"]},{"cell_type":"markdown","metadata":{"id":"dSvjuIa2LlOY"},"source":["Finalize the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3frSPd3Lj7i"},"outputs":[],"source":["#Non Sequential model as it has two different outputs\n","final_model = tf.keras.models.Model(inputs=model.input, #Pre-trained model input as input layer\n","                                    outputs=[label_output,bbox_output]) #Output layer added"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvXbMqx_sOCh"},"outputs":[],"source":["final_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"6KpqhepoiAfe"},"source":["Define function to calculate IoU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ww0PFGg6ql3C"},"outputs":[],"source":["def calculate_iou(y_true, y_pred):\n","    \n","    \n","    \"\"\"\n","    Input:\n","    Keras provides the input as numpy arrays with shape (batch_size, num_columns).\n","    \n","    Arguments:\n","    y_true -- first box, numpy array with format [x, y, width, height, conf_score]\n","    y_pred -- second box, numpy array with format [x, y, width, height, conf_score]\n","    x any y are the coordinates of the top left corner of each box.\n","    \n","    Output: IoU of type float32. (This is a ratio. Max is 1. Min is 0.)\n","    \n","    \"\"\"\n","\n","    \n","    results = []\n","    \n","    for i in range(0,y_true.shape[0]):\n","    \n","        # set the types so we are sure what type we are using\n","        y_true = np.array(y_true, dtype=np.float32)\n","        y_pred = np.array(y_pred, dtype=np.float32)\n","\n","        #print(y_true.shape)\n","        #print(y_pred.shape)\n","        # boxTrue\n","        x_boxTrue_tleft = y_true[i,0]  # numpy index selection\n","        y_boxTrue_tleft = y_true[i,1]\n","        boxTrue_width = y_true[i,2]\n","        boxTrue_height = y_true[i,3]\n","        area_boxTrue = (boxTrue_width * boxTrue_height)\n","\n","        # boxPred\n","        x_boxPred_tleft = y_pred[i,0]\n","        y_boxPred_tleft = y_pred[i,1]\n","        boxPred_width = y_pred[i,2]\n","        boxPred_height = y_pred[i,3]\n","        area_boxPred = (boxPred_width * boxPred_height)\n","\n","        # calculate the bottom right coordinates for boxTrue and boxPred\n","\n","        # boxTrue\n","        x_boxTrue_br = x_boxTrue_tleft + boxTrue_width\n","        y_boxTrue_br = y_boxTrue_tleft + boxTrue_height # Version 2 revision\n","\n","        # boxPred\n","        x_boxPred_br = x_boxPred_tleft + boxPred_width\n","        y_boxPred_br = y_boxPred_tleft + boxPred_height # Version 2 revision\n","\n","\n","        # calculate the top left and bottom right coordinates for the intersection box, boxInt\n","\n","        # boxInt - top left coords\n","        x_boxInt_tleft = np.max([x_boxTrue_tleft,x_boxPred_tleft])\n","        y_boxInt_tleft = np.max([y_boxTrue_tleft,y_boxPred_tleft]) # Version 2 revision\n","\n","        # boxInt - bottom right coords\n","        x_boxInt_br = np.min([x_boxTrue_br,x_boxPred_br])\n","        y_boxInt_br = np.min([y_boxTrue_br,y_boxPred_br]) \n","\n","        # Calculate the area of boxInt, i.e. the area of the intersection \n","        # between boxTrue and boxPred.\n","        # The np.max() function forces the intersection area to 0 if the boxes don't overlap.\n","        \n","        \n","        # Version 2 revision\n","        area_of_intersection = \\\n","        np.max([0,(x_boxInt_br - x_boxInt_tleft)]) * np.max([0,(y_boxInt_br - y_boxInt_tleft)])\n","\n","        iou = area_of_intersection / ((area_boxTrue + area_boxPred) - area_of_intersection)\n","\n","\n","        # This must match the type used in py_func\n","        iou = np.array(iou, dtype=np.float32)\n","        \n","        # append the result to a list at the end of each loop\n","        results.append(iou)\n","    \n","    # return the mean IoU score for the batch\n","    return np.mean(results)\n","\n","\n","\n","def IoU(y_true, y_pred):\n","    \n","    # Note: the type float32 is very important. It must be the same type as the output from\n","    # the python function above or you too may spend many late night hours \n","    # trying to debug and almost give up.\n","    \n","    iou = tf.py_function(calculate_iou, [y_true, y_pred], tf.float32)\n","\n","    return iou"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfVR7QqYLyNz"},"outputs":[],"source":["final_model.compile(optimizer='adam', \n","                    loss={'reg_op':'mse', 'class_op':'categorical_crossentropy'},\n","                    loss_weights={'reg_op':1, 'class_op':1},\n","                    metrics={'reg_op':[IoU], 'class_op':['accuracy']})"]},{"cell_type":"markdown","metadata":{"id":"-RQg_TeWL8CC"},"source":["Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLOR7EApL6cl"},"outputs":[],"source":["#Create train and test generator\n","batchsize = 64\n","train_generator = batch_generator(train_df, batch_size=batchsize) #batchsize can be changed\n","test_generator = batch_generator(test_df, batch_size=batchsize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGjtvmc_zaIh"},"outputs":[],"source":["final_model.fit(train_generator,\n","                epochs=5,\n","                steps_per_epoch= train_df.shape[0]//batchsize,\n","                validation_data=test_generator,\n","                validation_steps = test_df.shape[0]//batchsize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmqypNf28WGS"},"outputs":[],"source":["final_model.compile(optimizer='adam', \n","                    loss={'reg_op':'mse', 'class_op':'categorical_crossentropy'},\n","                    loss_weights={'reg_op':10, 'class_op':1},\n","                    metrics={'reg_op':[IoU], 'class_op':['accuracy']})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8xqseOz8fk0"},"outputs":[],"source":["final_model.fit(train_generator,\n","                epochs=10,\n","                initial_epoch=5,\n","                steps_per_epoch= train_df.shape[0]//batchsize,\n","                validation_data=test_generator,\n","                validation_steps = test_df.shape[0]//batchsize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5msq1-6tX3lJ"},"outputs":[],"source":["final_model.save('pet_dataset_localization.h5')"]},{"cell_type":"markdown","metadata":{"id":"RT_TJaPWX2PK"},"source":["#### Model Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyrMXl9EdiSt"},"outputs":[],"source":["def predict_and_draw(image_num, df):\n","\n","    #Load image\n","    img = tf.keras.preprocessing.image.load_img(df.loc[image_num, 'File'])\n","    w, h = img.size\n","\n","    #Prepare input for model\n","    #1. Resize image\n","    img_resized = img.resize((img_size, img_size))\n","    #2. Conver to array and make it a batch of 1\n","    input_array = tf.keras.preprocessing.image.img_to_array(img_resized)\n","    input_array = np.expand_dims(input_array, axis=0)\n","\n","    #3. Normalize image data\n","    input_array = tf.keras.applications.resnet50.preprocess_input(input_array)\n","\n","    #Prediction\n","    pred = final_model.predict(input_array)\n","    #Get classification and regression predictions\n","    label_pred, bbox_pred = pred[0][0], pred[1][0]\n","\n","    print('Class label prediction', label_pred)\n","    print('Boundary box prediction', bbox_pred)\n","\n","    #Get Label with highest probability\n","    pred_class = label_class_dict[np.argmax(label_pred)]\n","\n","    #Read actual label and bounding box\n","    act_class = df.loc[image_num, 'Class']\n","    xmin, ymin, xmax, ymax = df.loc[image_num, ['xmin', 'ymin', 'xmax', 'ymax']]\n","\n","    print('Actual Label :', act_class, '\\nPredicted Label: ', pred_class)\n","    \n","    #Draw bounding boxes - Actual (Red) and Predicted(Green)\n","    img = cv2.imread(df.loc[image_num, 'File'])\n","    \n","    #Draw actual bounding box - RED\n","    img = cv2.rectangle(img, (xmin, ymin), \n","                        (xmax, ymax), (0,0,255), 3)\n","    \n","    #Draw predicted bounding box\n","    img = cv2.rectangle(img, (int(bbox_pred[0]*w), int(bbox_pred[1]*h)), \n","                        (int((bbox_pred[0]+bbox_pred[2])*w), int((bbox_pred[1]+bbox_pred[3])*h)), (0,255,0), 3)\n","\n","    #Display the picture\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    plt.figure(figsize=(10,10))\n","    plt.imshow(img)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DH0bH4YiYZ4b"},"outputs":[],"source":["#Predict on Test Dataset\n","image_num = np.random.randint(0, test_df.shape[0])\n","predict_and_draw(image_num, test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoiHsbD5y3GP"},"outputs":[],"source":["img = tf.keras.preprocessing.image.load_img(test_df.loc[0, 'File'], target_size=(224,224))\n","img_array = tf.keras.preprocessing.image.img_to_array(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cq0tfsMpzBEU"},"outputs":[],"source":["img_array.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1eNVnlPzFRC"},"outputs":[],"source":["final_model.input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcC8prqqtezR"},"outputs":[],"source":["final_model.predict(img_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFZz1eZCzTeG"},"outputs":[],"source":["np.expand_dims(img_array, axis=0).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPwmXqgazOZ1"},"outputs":[],"source":["final_model.predict(np.expand_dims(img_array, axis=0))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["rDL5UxUs0nUu","KIN4CgX185lH"],"name":"1. Object Localization - Oxford IIIT Pet Dataset.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
